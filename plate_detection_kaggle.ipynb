{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('../input/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-14T11:13:09.428934Z","iopub.execute_input":"2022-01-14T11:13:09.429243Z","iopub.status.idle":"2022-01-14T11:13:09.823405Z","shell.execute_reply.started":"2022-01-14T11:13:09.429191Z","shell.execute_reply":"2022-01-14T11:13:09.822505Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/working/plates/'\nprint(os.listdir(data_root))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:09.825280Z","iopub.execute_input":"2022-01-14T11:13:09.825738Z","iopub.status.idle":"2022-01-14T11:13:09.831230Z","shell.execute_reply.started":"2022-01-14T11:13:09.825686Z","shell.execute_reply":"2022-01-14T11:13:09.830322Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Ячейкой ниже мы из нашей папки train сделали еще 2 папки (train и valid).\n## в папку valid закинули 1/6 всех файлов из train ","metadata":{}},{"cell_type":"code","source":"import shutil \nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-01-14T11:13:09.832746Z","iopub.execute_input":"2022-01-14T11:13:09.833340Z","iopub.status.idle":"2022-01-14T11:13:09.861690Z","shell.execute_reply.started":"2022-01-14T11:13:09.833286Z","shell.execute_reply":"2022-01-14T11:13:09.860561Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:09.864887Z","iopub.execute_input":"2022-01-14T11:13:09.865320Z","iopub.status.idle":"2022-01-14T11:13:10.572677Z","shell.execute_reply.started":"2022-01-14T11:13:09.865272Z","shell.execute_reply":"2022-01-14T11:13:10.571863Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!ls train","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:10.577977Z","iopub.execute_input":"2022-01-14T11:13:10.578243Z","iopub.status.idle":"2022-01-14T11:13:11.278302Z","shell.execute_reply.started":"2022-01-14T11:13:10.578194Z","shell.execute_reply":"2022-01-14T11:13:11.277549Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":" ## torchvision.datasets.ImageFolder\n ImageFolder -  итерируется по директориям, получает картинки, формирует из этих картинок пару (тензор и метка картинки) и их можно уже передавать в нейронную сеть. Метки это название папки, в которых лежат изображения.\n \nНо если ImageFolder использовать без второго аргумента, то он вернет изображения, которые открыты через библиотеку PIL (Python Image Library) -- это будут не pytorch тензоры. И для этого нам нужно сделать некоторую трансформацию этих изображений, что бы получить тензоры\n\n## transforms по валидации \n\nБудем делать три трансформации. Мы их вместе соединим с помощью объекта \"transforms.Compose\", то есть все трансформации будут подряд проделываться\n\n1. Ужмём до размера 224 на 224. \n   ResNet, который мы используем -- инвариантен к размерам, но объекты для обучения должны быть одинакового размера. Мы выбрали 224 на 224 -- ибо это размер изображений из датасета ImageNet, на котором предобучен наш ResNet.\n2. Превращаем изображение в тензор \n3. Нормировка изображений. Отнимаем и делим наши каналы именно на такие константы. Именно так был отнормирован изначальный ResNet, если посмотреть его документацию\n\n## transforms по train\n\nпримерним аугментацию (способы из одной картинки сделать сразу много), есть намного больше видов аугментацию, пока ограничемся только этим. \n1. RandomResizedCrop - вырезает рандомные участки изображения и приводит их к размеру 224 на 224.\n2. RandomHorizontalFlip - отзеркаливание относительно горизонтали\n3. трансформация в тензор\n4. нормировка\n\n## torch.utils.data.DataLoader\n\nDataLoader - объединит тензоры из изображений в батчи из таких изображений. Потому что нам хочется несколько изображений подавать в нейронку и считать некоторую лосс-функцию сразу по нескольким.\n\n num_workers -- это количество тредов (потоков), которые будут выполнять вот эти трансформации.","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\nfrom torchvision import transforms, models\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:11.281995Z","iopub.execute_input":"2022-01-14T11:13:11.282249Z","iopub.status.idle":"2022-01-14T11:13:11.294557Z","shell.execute_reply.started":"2022-01-14T11:13:11.282202Z","shell.execute_reply":"2022-01-14T11:13:11.293752Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(train_dataset) # 32 / 8 = 4","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:11.296076Z","iopub.execute_input":"2022-01-14T11:13:11.296559Z","iopub.status.idle":"2022-01-14T11:13:11.307908Z","shell.execute_reply.started":"2022-01-14T11:13:11.296502Z","shell.execute_reply":"2022-01-14T11:13:11.307075Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"#### Можем посмотреть, как выглядит одно изображение из train_dataloader.\n#### Что бы его визуализировать, нужно применить обратную нормирвку к нашему тензору.\n#### .permute(1, 2, 0) - меняет каналы местами\n","metadata":{}},{"cell_type":"code","source":"X_batch, y_batch = next(iter(train_dataloader))\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nplt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean);","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:11.309336Z","iopub.execute_input":"2022-01-14T11:13:11.309852Z","iopub.status.idle":"2022-01-14T11:13:11.956862Z","shell.execute_reply.started":"2022-01-14T11:13:11.309585Z","shell.execute_reply":"2022-01-14T11:13:11.955948Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### Все тоже самое, но в виде фкнкции. Полезно для того, что бы отследить результат аугментации. Например,на 3 карнтинки, у нас изображон в основном только стол, вместо тарелки и это не так хорошо, можно поменять параметр в RandomResizedCrop","metadata":{}},{"cell_type":"code","source":"def show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:11.961715Z","iopub.execute_input":"2022-01-14T11:13:11.964303Z","iopub.status.idle":"2022-01-14T11:13:14.736567Z","shell.execute_reply.started":"2022-01-14T11:13:11.964235Z","shell.execute_reply":"2022-01-14T11:13:14.735516Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step() # для планировщика даем команду, что произшла 1 эпоха \n                \"\"\"нужно для того, что бы зафиксировать слои нормализации и просто на всякий случай\"\"\"\n                model.train()  # Set model to training mode, \n            else:\n                dataloader = val_dataloader\n                \"\"\"что бы во время валидации не изменялась нейронка\"\"\"\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad() # обнуляем градиент, что бы он не накапливался каждый раз\n\n                # forward and backward. Активируем все градиенты(веса), которые не заморожены \n                with torch.set_grad_enabled(phase == 'train'):\n                    \"\"\"Считаем предикшн модели, это не вероятности, а просто аквации нейронов\"\"\"\n                    preds = model(inputs) \n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1) # берем нейрон с максимальной активацией\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss / len(dataloader)\n            epoch_acc = running_acc / len(dataloader)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:14.738407Z","iopub.execute_input":"2022-01-14T11:13:14.739019Z","iopub.status.idle":"2022-01-14T11:13:14.765813Z","shell.execute_reply.started":"2022-01-14T11:13:14.738959Z","shell.execute_reply":"2022-01-14T11:13:14.764904Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model.fc.in_features","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:14.771195Z","iopub.execute_input":"2022-01-14T11:13:14.774501Z","iopub.status.idle":"2022-01-14T11:13:14.786658Z","shell.execute_reply.started":"2022-01-14T11:13:14.774430Z","shell.execute_reply":"2022-01-14T11:13:14.785713Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"\"\"\"В pytorch есть много разных ResNet. resnet18 -  \"18\" говорит о том, что там 18 слоёв \npretrained = True - то есть нам нужны веса, полученные вследствие обучения этого ResNet, на датасете ImageNet.\n\"\"\"\nmodel = models.resnet18(pretrained=True)\n\n\"\"\"отключить рассчет градиента для всех слоев сети\nсеть хорошо предобучена и мы не хотим, что бы веса менялись во время обучения\n\"\"\"\nfor param in model.parameters():\n    param.requires_grad = False\n\n\"\"\"меняем последний слой, вместо 1000 классов, как в оригинале, у нас будет 2\nпо умолчанию градиенты там всегда считаются, так что это единственный слой, который обучается\n\"\"\"\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\n\"\"\"переложили модель на видеокарту\"\"\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss() # функция активации \noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3) # метод оптимизации\n\n\n\"\"\"некоторый планировщик, scheduler, который будет снижать шаг градиентного спуска во время обучения.\nЗатухание LR в 0,1 раза каждые 7 эпох\nЕсть некоторая эвристика, что, с течением обучения, нужно уменьшать шаг градиентного спуска.\"\"\"\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:14.791943Z","iopub.execute_input":"2022-01-14T11:13:14.793982Z","iopub.status.idle":"2022-01-14T11:13:15.141789Z","shell.execute_reply.started":"2022-01-14T11:13:14.793917Z","shell.execute_reply":"2022-01-14T11:13:15.141062Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_model(model, loss, optimizer, scheduler, num_epochs=100);","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:13:15.143232Z","iopub.execute_input":"2022-01-14T11:13:15.143819Z","iopub.status.idle":"2022-01-14T11:14:30.661310Z","shell.execute_reply.started":"2022-01-14T11:13:15.143719Z","shell.execute_reply":"2022-01-14T11:14:30.660214Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### как вы помните, ImageFolder -- он принимал на вход путь к папке, в которой есть некоторые папки с классами. У нас был путь к папке \"train\", а внутри папки \"train\" был cleaned и dirty, и к сожалению, ImageFolder не сможет вам обработать путь к папке, в которой уже сразу лежат изображения, а у нас в тестовой директории уже сразу лежат изображения. И поэтому нам нужно сделать некоторый хак, то есть скопировать всю папку в тест -- в папку \"test/unknown\".","metadata":{}},{"cell_type":"code","source":"test_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:30.662802Z","iopub.execute_input":"2022-01-14T11:14:30.663129Z","iopub.status.idle":"2022-01-14T11:14:30.806042Z","shell.execute_reply.started":"2022-01-14T11:14:30.663066Z","shell.execute_reply":"2022-01-14T11:14:30.805002Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Вторая проблема - это то, что мы не знаем, какие ID, какие названия изображения у нас генерируется, когда мы просим у DataLoader -- \"дай нам следующий батч\". Они по алфавиту идут или по дате создания, или просто случайным образом -- непонятно. Поэтому нам нужно переписать немножко ImageFolder, чтобы он нам отдавал не просто tuple, с самим изображением и его меткой, а ещё, чтобы он отдавал имя, ну, либо -- путь к изображению.","metadata":{}},{"cell_type":"code","source":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('/kaggle/working/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:30.807548Z","iopub.execute_input":"2022-01-14T11:14:30.807980Z","iopub.status.idle":"2022-01-14T11:14:30.820332Z","shell.execute_reply.started":"2022-01-14T11:14:30.807800Z","shell.execute_reply":"2022-01-14T11:14:30.819425Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"test_dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:30.821811Z","iopub.execute_input":"2022-01-14T11:14:30.822229Z","iopub.status.idle":"2022-01-14T11:14:30.827900Z","shell.execute_reply.started":"2022-01-14T11:14:30.822063Z","shell.execute_reply":"2022-01-14T11:14:30.827089Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### смотрим как наша модель отработала на test, сохраняя уже вероятности в test_predictions","metadata":{}},{"cell_type":"code","source":"model.eval() ## фиксируем модельку \n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader): ### labels - всегда будет None\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:30.829270Z","iopub.execute_input":"2022-01-14T11:14:30.829744Z","iopub.status.idle":"2022-01-14T11:14:34.353364Z","shell.execute_reply.started":"2022-01-14T11:14:30.829696Z","shell.execute_reply":"2022-01-14T11:14:34.351735Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:34.354678Z","iopub.execute_input":"2022-01-14T11:14:34.355000Z","iopub.status.idle":"2022-01-14T11:14:36.609989Z","shell.execute_reply.started":"2022-01-14T11:14:34.354953Z","shell.execute_reply":"2022-01-14T11:14:36.609160Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:36.611503Z","iopub.execute_input":"2022-01-14T11:14:36.616776Z","iopub.status.idle":"2022-01-14T11:14:36.626271Z","shell.execute_reply.started":"2022-01-14T11:14:36.616714Z","shell.execute_reply":"2022-01-14T11:14:36.625292Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:36.632234Z","iopub.execute_input":"2022-01-14T11:14:36.632686Z","iopub.status.idle":"2022-01-14T11:14:36.662808Z","shell.execute_reply.started":"2022-01-14T11:14:36.632629Z","shell.execute_reply":"2022-01-14T11:14:36.662235Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:36.663945Z","iopub.execute_input":"2022-01-14T11:14:36.664217Z","iopub.status.idle":"2022-01-14T11:14:36.670032Z","shell.execute_reply.started":"2022-01-14T11:14:36.664159Z","shell.execute_reply":"2022-01-14T11:14:36.669197Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"!rm -rf train val test","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:14:36.671801Z","iopub.execute_input":"2022-01-14T11:14:36.672436Z","iopub.status.idle":"2022-01-14T11:14:37.394453Z","shell.execute_reply.started":"2022-01-14T11:14:36.672385Z","shell.execute_reply":"2022-01-14T11:14:37.393522Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}